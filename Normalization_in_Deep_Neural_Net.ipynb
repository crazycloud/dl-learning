{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYysPXvO4zjZaLuMC6NJia",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crazycloud/dl-learning/blob/main/Normalization_in_Deep_Neural_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. Normalization significantly reduces the training time in feedforward neural networks by addressing the internal covariate shift problem during training.\n",
        "\n",
        "\n",
        "From [Layer Normalization paper](https://arxiv.org/pdf/1607.06450.pdf)\n",
        "\n",
        "\n",
        "\n",
        "> *One of the challenges of deep learning is that the gradients with respect to the weights in one layer are highly dependent on the outputs of the neurons in the previous layer especially if these outputs change in a highly correlated way. Batch normalization [Ioffe and Szegedy, 2015] was proposed to reduce such undesirable “covariate shift”. The method normalizes the summed inputs to each hidden unit over the training cases. Specifically, for the i th summed input in the l th layer, the batch normalization method rescales the summed inputs according to their variances under the distribution of the data*\n",
        "\n",
        "\n",
        "## What is covariate shift?\n",
        "\n",
        "The covariate shift problem refers to a phenomenon that can occur during the training of neural networks. It arises when the distribution of the input data (activations) to each layer of the network changes during training. This **change in the distribution of input data at each layer can make it more challenging** for the network to converge and learn effectively.\n",
        "\n",
        "This change in distribution can be problematic for the network's learning process because the model might have difficulty in generalizing well to unseen data. The network might converge slowly, or it could even fail to converge altogether. As a result, the training process becomes less efficient and may require additional iterations or adjustments in learning rate to achieve good performance.\n",
        "\n",
        "![Covariate shift](https://raw.githubusercontent.com/crazycloud/dl-learning/main/assets/covariate.png)\n",
        "\n",
        "## Batch Normalization\n",
        "\n",
        "For each layer in the neural network, batch normalization normalizes the activations by adjusting them to have a standardized mean and variance.\n",
        "\n",
        "- Normalization within a batch: During the training process, the input data is divided into batches. For each batch, **the mean and variance of the activations within that batch are computed**. The activations are then normalized based on the statistics (mean and variance) of the current batch.\n",
        "\n",
        "- Learnable parameters: Batch normalization introduces learnable parameters, **scale, and shift parameters, which allow the network to learn the appropriate scaling and shifting** for the normalized activations.\n",
        "\n",
        "\n",
        "By normalizing the activations within each batch, the network becomes less sensitive to the changes in the input data distribution that occur during training. This helps in faster convergence, better generalization, and overall more stable training of deep neural networks.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Cl_7tgEUzhcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Batch Normalization](https://raw.githubusercontent.com/crazycloud/dl-learning/main/assets/batchnorm.png)"
      ],
      "metadata": {
        "id": "jU-zbwVBMvxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BatchNorm(nn.Module):\n",
        "    def __init__(self, emb, eps=1e-5, momentum=0.1):\n",
        "        super(BatchNorm, self).__init__()\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum # weightage on current mean/variance vs running value\n",
        "        self.register_buffer('running_mean', torch.zeros(emb))\n",
        "        self.register_buffer('running_var', torch.ones(emb))\n",
        "        self.weight = nn.Parameter(torch.ones(emb))  # Learnable scale parameter\n",
        "        self.bias = nn.Parameter(torch.zeros(emb))   # Learnable shift parameter\n",
        "\n",
        "    def forward(self, a):\n",
        "        if self.training:\n",
        "            mean = a.mean(dim=0, keepdim=True)\n",
        "            variance = a.var(dim=0, unbiased=False, keepdim=True)\n",
        "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.squeeze() # running mean across multiple batches\n",
        "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * variance.squeeze() # running variance across multiple batches\n",
        "        else:\n",
        "            mean = self.running_mean.view(1, -1)\n",
        "            variance = self.running_var.view(1, -1)\n",
        "\n",
        "        normalized_input = (a - mean) / torch.sqrt(variance + self.eps)\n",
        "        output = self.weight * normalized_input + self.bias\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "loPFZp6v1kOO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Layer Normalization\n",
        "\n",
        "Layer normalization is another technique used to normalize activations in neural networks. While batch normalization normalizes activations within a batch, **layer normalization normalizes activations across the features within a single sample**, regardless of the batch size.\n",
        "\n",
        "LayerNorm(LN) is typically used in NLP tasks. The statistics of NLP data across the batch dimension exhibit large fluctuations compared to CV task throughout training. This results in instability when using BatchNorm(BN). The Attention is All you Need paper used LN as the default normalization scheme.\n",
        "\n",
        "\n",
        "![Alt text](https://i.stack.imgur.com/E3104.png)"
      ],
      "metadata": {
        "id": "hU-gTglkxSHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Layer Normalization](https://raw.githubusercontent.com/crazycloud/dl-learning/main/assets/layernorm.png)"
      ],
      "metadata": {
        "id": "EFswH7r-y-KD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(emb))  # Learnable scale parameter\n",
        "        self.bias = nn.Parameter(torch.zeros(emb))   # Learnable shift parameter\n",
        "\n",
        "    def _norm(self, x):\n",
        "        mean = a.mean(-1, keepdim=True)\n",
        "        variance = a.var(dim=-1, unbiased=False, keepdim=True)\n",
        "        return (a - mean) / torch.sqrt(variance + self.eps)\n",
        "\n",
        "    def forward(self, a):\n",
        "        return self.weight * self._norm(a) + self.bias"
      ],
      "metadata": {
        "id": "-6NVx4dm2CLN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Root Mean Square Normalization (RMSNorm)\n",
        "\n",
        "\n",
        "The **llama and llama2 architecture uses RMNorm** instead of LayerNorm.\n",
        "\n",
        "LayerNorm introduces some computational overhead. While it might be small for shallow neural networks with few norm layers, this overhead becomes substantial as underlying models grow larger and deeper. Consequently, the efficiency benefit of faster and more stable training enabled by LayerNorm can be offset by the increased cost per training step.\n",
        "\n",
        "One main finding behind RMSNorm is that **mean normalization in LayerNorm doesn't reduce the variance of hidden states or model gradients**. It has little impact on the stabilization.\n",
        "\n",
        "**RMSNorm reduces the amount of computation and increases efficiency over LayerNorm**. Despite the\n",
        "simpler formulation, the RMS normalizer helps stabilize the magnitude of layer activations, ensuring invariance to the re-scaling of both weights and datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "pHEOZJ04y-cS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Batch Normalization](https://raw.githubusercontent.com/crazycloud/dl-learning/main/assets/rmsnorm.png)"
      ],
      "metadata": {
        "id": "alWAlwWmNR7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the [Root Mean Square Layer Normalization paper](https://arxiv.org/pdf/1910.07467.pdf) -\n",
        "\n",
        "\n",
        "> *A well-known explanation of the success of LayerNorm is its re-centering and re-scaling invariance property. The former enables the model to be insensitive to shift noises on both inputs and weights, and the latter keeps the output representations intact when both inputs and weights are randomly scaled. In this paper, we hypothesize that the re-scaling invariance is the reason for success of LayerNorm, rather than re-centering invariance*\n",
        "\n",
        "\n",
        "> *Intuitively, RMSNorm simplifies LayerNorm by totally removing the mean statistic at the cost of sacrificing the invariance that mean normalization affords. When the mean of summed inputs is zero, RMSNorm is exactly equal to LayerNorm. Although RMSNorm does not re-center the summed inputs as in LayerNorm, we demonstrate through experiments that this property is not fundamental to the success of LayerNorm, and that RMSNorm is similarly or more effective.*\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jXjALIW9N7cZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = torch.nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, a):\n",
        "        return a * torch.rsqrt(a.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(a.float()).type_as(a)\n",
        "        return output * self.weight\n"
      ],
      "metadata": {
        "id": "7ZKbcLlG2GQ0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tvxF7dkV2NnX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}